{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d55f0c9",
   "metadata": {},
   "source": [
    "# Notebook 1 - Carga de Datos\n",
    "\n",
    "Este notebook deja el dataset listo para N2 con enfoque `fail-fast`: si un control critico falla, se detiene la ejecucion.\n",
    "\n",
    "Flujo del notebook:\n",
    "1. Configuracion y parametros operativos.\n",
    "2. Carga del parquet base y validaciones tecnicas.\n",
    "3. Resumen de cobertura y validacion de benchmark (SPY).\n",
    "4. Exportes finales + checklist de readiness para N2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca671e",
   "metadata": {},
   "source": [
    "## 1) Configuracion del Notebook\n",
    "Se definen parametros del enunciado, rutas de artefactos y esquema esperado.\n",
    "Por que: mantener trazabilidad y consistencia desde N1 hasta N5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594e3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 1: configuracion global, esquema esperado y utilidades fail-fast.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import yfinance as yf\n",
    "\n",
    "# Parametros maestros del enunciado para N1-N5.\n",
    "PARAMS = {\n",
    "    \"initial_capital\": 250000,\n",
    "    \"backtest_start\": \"2015-01-01\",\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    \"universe_rule\": \"in_sp500_point_in_time_last_13_months\",\n",
    "    \"transaction_fee_rate\": 0.0023,\n",
    "    \"min_fee_per_order\": 23.0,\n",
    "}\n",
    "\n",
    "# Rutas de entrada/salida usadas por el pipeline de datos.\n",
    "INPUT_PARQUET = \"data/raw/sp500_history.parquet\"\n",
    "OUT_CANONICAL = \"data/processed/sp500_canonical.parquet\"\n",
    "OUT_SPY_PARQUET = \"data/raw/spy_yfinance.parquet\"\n",
    "OUT_CHECKS_CSV = \"data/processed/n1_data_quality_checks.csv\"\n",
    "OUT_COVERAGE_CSV = \"data/processed/n1_coverage_summary.csv\"\n",
    "OUT_RUNTIME_PARAMS_CSV = \"data/processed/n1_runtime_params.csv\"\n",
    "\n",
    "# Esquema esperado para asegurar consistencia entre notebooks.\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"date\",\n",
    "    \"symbol\",\n",
    "    \"assetid\",\n",
    "    \"security_name\",\n",
    "    \"sector\",\n",
    "    \"industry\",\n",
    "    \"subsector\",\n",
    "    \"in_sp500\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"unadjusted_close\",\n",
    "]\n",
    "\n",
    "# Columnas que deben ser numericas para calcular senales y costes despues.\n",
    "NUMERIC_COLUMNS = [\n",
    "    \"in_sp500\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"unadjusted_close\",\n",
    "]\n",
    "\n",
    "# Campos minimos para mantener integridad del backtest.\n",
    "CRITICAL_COLUMNS = [\n",
    "    \"date\",\n",
    "    \"symbol\",\n",
    "    \"in_sp500\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "]\n",
    "\n",
    "\n",
    "def fail_if(condition: bool, message: str) -> None:\n",
    "    \"\"\"Aplica fail-fast para cortar ejecucion ante datos no confiables.\"\"\"\n",
    "    if condition:\n",
    "        raise ValueError(message)\n",
    "\n",
    "\n",
    "def record_check(\n",
    "    results: list,\n",
    "    check_name: str,\n",
    "    failed: bool,\n",
    "    details: str,\n",
    ") -> None:\n",
    "    \"\"\"Guarda un check tecnico y aborta el pipeline cuando falla.\"\"\"\n",
    "    passed = not failed\n",
    "    results.append({\"check\": check_name, \"passed\": passed, \"details\": details})\n",
    "    fail_if(failed, f\"[{check_name}] {details}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e87181",
   "metadata": {},
   "source": [
    "## 2) Carga y Validacion Tecnica del Dataset Base\n",
    "Se carga `sp500_history.parquet` y se ejecutan checks de integridad, tipos, nulos, duplicados y coherencia OHLCV.\n",
    "Por que: evitar propagar errores a N2/N3/N4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f252b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 2: carga del parquet base y controles tecnicos de calidad.\n",
    "def load_input_data(\n",
    "    input_parquet: str,\n",
    "    expected_columns: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Carga el parquet base y normaliza tipos clave para evitar errores aguas abajo.\"\"\"\n",
    "    input_exists = pd.io.common.file_exists(input_parquet)\n",
    "    fail_if(\n",
    "        not input_exists,\n",
    "        f\"Input parquet no encontrado: {input_parquet}\",\n",
    "    )\n",
    "\n",
    "    # Seleccion explicita de columnas para detectar schema drift temprano.\n",
    "    df = pd.read_parquet(\n",
    "        input_parquet,\n",
    "        columns=expected_columns,\n",
    "        engine=\"pyarrow\",\n",
    "    )\n",
    "\n",
    "    # Tipado base para joins/orden temporal determinista en todos los notebooks.\n",
    "    df.columns = [str(col).lower() for col in df.columns]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"symbol\"] = df[\"symbol\"].astype(\"string\")\n",
    "    df = df.sort_values([\"date\", \"symbol\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_data_quality_checks(\n",
    "    df: pd.DataFrame,\n",
    "    expected_columns: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Ejecuta controles de estructura y coherencia para habilitar el backtest.\"\"\"\n",
    "    validation_rows = []\n",
    "\n",
    "    # 1) Integridad de esquema.\n",
    "    missing_columns = sorted(set(expected_columns) - set(df.columns))\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"required_columns_present\",\n",
    "        bool(missing_columns),\n",
    "        f\"missing_columns={missing_columns}\",\n",
    "    )\n",
    "\n",
    "    # 2) Tipos esenciales.\n",
    "    date_is_datetime = pd.api.types.is_datetime64_any_dtype(df[\"date\"])\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"date_is_datetime\",\n",
    "        not date_is_datetime,\n",
    "        \"date dtype invalido\",\n",
    "    )\n",
    "\n",
    "    symbol_is_string = pd.api.types.is_string_dtype(df[\"symbol\"])\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"symbol_is_string\",\n",
    "        not symbol_is_string,\n",
    "        \"symbol dtype invalido\",\n",
    "    )\n",
    "\n",
    "    numeric_type_failures = [\n",
    "        col for col in NUMERIC_COLUMNS\n",
    "        if not pd.api.types.is_numeric_dtype(df[col])\n",
    "    ]\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"numeric_columns_dtype\",\n",
    "        bool(numeric_type_failures),\n",
    "        f\"invalid_numeric_dtype={numeric_type_failures}\",\n",
    "    )\n",
    "\n",
    "    # 3) Unicidad y nulos criticos.\n",
    "    duplicate_count = int(df.duplicated([\"date\", \"symbol\"]).sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"duplicate_date_symbol\",\n",
    "        duplicate_count > 0,\n",
    "        f\"duplicate_rows={duplicate_count}\",\n",
    "    )\n",
    "\n",
    "    critical_null_count = int(df[CRITICAL_COLUMNS].isna().sum().sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"critical_nulls\",\n",
    "        critical_null_count > 0,\n",
    "        f\"critical_null_count={critical_null_count}\",\n",
    "    )\n",
    "\n",
    "    # 4) Reglas economicas minimas para OHLCV.\n",
    "    price_array = df[[\"open\", \"high\", \"low\", \"close\"]].to_numpy(dtype=float)\n",
    "    invalid_price_mask = (~np.isfinite(price_array)) | (price_array <= 0)\n",
    "    invalid_price_count = int(invalid_price_mask.any(axis=1).sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"prices_positive\",\n",
    "        invalid_price_count > 0,\n",
    "        f\"invalid_price_rows={invalid_price_count}\",\n",
    "    )\n",
    "\n",
    "    volume_array = df[\"volume\"].to_numpy(dtype=float)\n",
    "    invalid_volume_mask = (~np.isfinite(volume_array)) | (volume_array < 0)\n",
    "    invalid_volume_count = int(invalid_volume_mask.sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"volume_non_negative\",\n",
    "        invalid_volume_count > 0,\n",
    "        f\"invalid_volume_rows={invalid_volume_count}\",\n",
    "    )\n",
    "\n",
    "    high_inconsistency = df[\"high\"] < df[[\"open\", \"close\", \"low\"]].max(axis=1)\n",
    "    low_inconsistency = df[\"low\"] > df[[\"open\", \"close\", \"high\"]].min(axis=1)\n",
    "    invalid_ohlc_count = int((high_inconsistency | low_inconsistency).sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"ohlc_consistency\",\n",
    "        invalid_ohlc_count > 0,\n",
    "        f\"invalid_ohlc_rows={invalid_ohlc_count}\",\n",
    "    )\n",
    "\n",
    "    # 5) Bandera point-in-time del universo en dominio binario.\n",
    "    invalid_in_sp500_count = int((~df[\"in_sp500\"].isin([0, 1])).sum())\n",
    "    record_check(\n",
    "        validation_rows,\n",
    "        \"in_sp500_binary\",\n",
    "        invalid_in_sp500_count > 0,\n",
    "        f\"invalid_in_sp500_rows={invalid_in_sp500_count}\",\n",
    "    )\n",
    "\n",
    "    checks_df = pd.DataFrame(validation_rows)\n",
    "    return checks_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a207f",
   "metadata": {},
   "source": [
    "## 3) Cobertura del Universo y Benchmark\n",
    "Se resume cobertura temporal/universo y se descarga/valida SPY con fallback local.\n",
    "Por que: asegurar comparativa con benchmark y continuidad operativa offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76625fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque 3: cobertura historica del universo y benchmark SPY.\n",
    "def build_coverage_summary(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Resume cobertura temporal y amplitud del universo historico.\"\"\"\n",
    "    date_min = df[\"date\"].min()\n",
    "    date_max = df[\"date\"].max()\n",
    "    total_symbols = int(df[\"symbol\"].nunique())\n",
    "\n",
    "    last_data_date = date_max\n",
    "    symbols_on_last_date = int(\n",
    "        df.loc[df[\"date\"] == last_data_date, \"symbol\"].nunique()\n",
    "    )\n",
    "\n",
    "    # Activo/inactivo se define por presencia en la ultima fecha disponible.\n",
    "    last_date_by_symbol = df.groupby(\"symbol\", observed=True)[\"date\"].max()\n",
    "    active_symbols = int((last_date_by_symbol == last_data_date).sum())\n",
    "    inactive_symbols = int(total_symbols - active_symbols)\n",
    "\n",
    "    # Conteo diario point-in-time de miembros del S&P500.\n",
    "    sp500_daily_count = (\n",
    "        df.loc[df[\"in_sp500\"] == 1]\n",
    "        .groupby(\"date\", observed=True)[\"symbol\"]\n",
    "        .nunique()\n",
    "        .rename(\"sp500_members\")\n",
    "    )\n",
    "\n",
    "    coverage_summary = {\n",
    "        \"date_min\": date_min.date().isoformat(),\n",
    "        \"date_max\": date_max.date().isoformat(),\n",
    "        \"total_rows\": int(len(df)),\n",
    "        \"total_symbols\": total_symbols,\n",
    "        \"symbols_on_last_date\": symbols_on_last_date,\n",
    "        \"active_symbols\": active_symbols,\n",
    "        \"inactive_symbols\": inactive_symbols,\n",
    "        \"sp500_members_min\": int(sp500_daily_count.min()),\n",
    "        \"sp500_members_median\": float(sp500_daily_count.median()),\n",
    "        \"sp500_members_max\": int(sp500_daily_count.max()),\n",
    "    }\n",
    "    return coverage_summary\n",
    "\n",
    "\n",
    "def _normalize_yfinance_columns(spy_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convierte columnas de yfinance a minusculas y snake_case.\"\"\"\n",
    "    if isinstance(spy_raw.columns, pd.MultiIndex):\n",
    "        spy_raw.columns = [\n",
    "            str(col[0]).lower().replace(\" \", \"_\")\n",
    "            for col in spy_raw.columns\n",
    "        ]\n",
    "    else:\n",
    "        spy_raw.columns = [\n",
    "            str(col).lower().replace(\" \", \"_\")\n",
    "            for col in spy_raw.columns\n",
    "        ]\n",
    "    return spy_raw\n",
    "\n",
    "\n",
    "def download_benchmark(\n",
    "    benchmark_ticker: str,\n",
    "    start_date: str,\n",
    "    out_spy_parquet: str,\n",
    ") -> tuple[pd.DataFrame, dict, str]:\n",
    "    \"\"\"Descarga SPY y aplica fallback local para permitir reejecucion offline.\"\"\"\n",
    "    spy_source = \"yfinance_download\"\n",
    "    try:\n",
    "        spy_raw = yf.download(\n",
    "            tickers=benchmark_ticker,\n",
    "            start=start_date,\n",
    "            progress=False,\n",
    "            auto_adjust=False,\n",
    "        )\n",
    "        fail_if(spy_raw.empty, \"No se pudo descargar SPY desde yfinance.\")\n",
    "\n",
    "        spy_raw = _normalize_yfinance_columns(spy_raw)\n",
    "        spy_df = spy_raw.reset_index()\n",
    "        spy_df.columns = [\n",
    "            str(col).lower().replace(\" \", \"_\")\n",
    "            for col in spy_df.columns\n",
    "        ]\n",
    "        spy_df[\"date\"] = pd.to_datetime(spy_df[\"date\"], errors=\"coerce\")\n",
    "        spy_df[\"symbol\"] = benchmark_ticker\n",
    "    except Exception as exc:\n",
    "        spy_source = \"local_parquet_fallback\"\n",
    "        fallback_exists = pd.io.common.file_exists(out_spy_parquet)\n",
    "        fail_if(\n",
    "            not fallback_exists,\n",
    "            \"No se pudo descargar SPY y no existe fallback local. \"\n",
    "            f\"Error original: {exc}\",\n",
    "        )\n",
    "        spy_df = pd.read_parquet(out_spy_parquet, engine=\"pyarrow\")\n",
    "\n",
    "    # Columnas minimas requeridas para benchmark en notebooks posteriores.\n",
    "    required_columns = [\"date\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    missing_columns = sorted(set(required_columns) - set(spy_df.columns))\n",
    "\n",
    "    spy_check = {\n",
    "        \"check\": \"spy_required_columns_present\",\n",
    "        \"passed\": not bool(missing_columns),\n",
    "        \"details\": f\"missing_columns={missing_columns}\",\n",
    "    }\n",
    "    fail_if(\n",
    "        bool(missing_columns),\n",
    "        f\"[spy_required_columns_present] {spy_check['details']}\",\n",
    "    )\n",
    "\n",
    "    spy_df[\"date\"] = pd.to_datetime(spy_df[\"date\"], errors=\"coerce\")\n",
    "    spy_df = spy_df[required_columns].sort_values(\"date\").reset_index(drop=True)\n",
    "    spy_df.to_parquet(out_spy_parquet, engine=\"pyarrow\", index=False)\n",
    "\n",
    "    return spy_df, spy_check, spy_source\n",
    "\n",
    "\n",
    "def export_artifacts(\n",
    "    canonical_df: pd.DataFrame,\n",
    "    checks_df: pd.DataFrame,\n",
    "    coverage_summary: dict,\n",
    "    runtime_params: dict,\n",
    "    out_canonical: str,\n",
    "    out_checks_csv: str,\n",
    "    out_coverage_csv: str,\n",
    "    out_runtime_csv: str,\n",
    ") -> dict:\n",
    "    \"\"\"Exporta datasets y tablas de control para continuidad N2-N5.\"\"\"\n",
    "    canonical_df.to_parquet(out_canonical, engine=\"pyarrow\", index=False)\n",
    "    checks_df.to_csv(out_checks_csv, index=False)\n",
    "    pd.DataFrame([coverage_summary]).to_csv(out_coverage_csv, index=False)\n",
    "\n",
    "    runtime_df = pd.DataFrame(\n",
    "        [{\"parameter\": key, \"value\": value} for key, value in runtime_params.items()]\n",
    "    )\n",
    "    runtime_df.to_csv(out_runtime_csv, index=False)\n",
    "\n",
    "    artifacts = {\n",
    "        \"canonical_exists\": pd.io.common.file_exists(out_canonical),\n",
    "        \"benchmark_exists\": pd.io.common.file_exists(OUT_SPY_PARQUET),\n",
    "        \"checks_csv_exists\": pd.io.common.file_exists(out_checks_csv),\n",
    "        \"coverage_csv_exists\": pd.io.common.file_exists(out_coverage_csv),\n",
    "        \"runtime_params_csv_exists\": pd.io.common.file_exists(out_runtime_csv),\n",
    "    }\n",
    "    return artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4af49d",
   "metadata": {},
   "source": [
    "## 4) Orquestacion y Exportes de N1\n",
    "Se ejecuta el pipeline completo, se exportan artefactos y se valida readiness para N2.\n",
    "Por que: dejar un punto de entrada estable y auditable para el siguiente notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5141b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA READINESS: PASS. El proyecto puede continuar en Notebook_2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_critical_checks_passed</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>canonical_dataset_created</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>benchmark_dataset_created</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checks_csv_created</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coverage_csv_created</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>runtime_params_csv_created</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         item  passed\n",
       "0  all_critical_checks_passed    True\n",
       "1   canonical_dataset_created    True\n",
       "2   benchmark_dataset_created    True\n",
       "3          checks_csv_created    True\n",
       "4        coverage_csv_created    True\n",
       "5  runtime_params_csv_created    True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bloque 4: orquestacion, exportes y checklist de readiness.\n",
    "def run_notebook_1() -> dict:\n",
    "    \"\"\"Orquesta todo N1 y devuelve objetos de control para inspeccion.\"\"\"\n",
    "    # 1) Carga y validacion del dataset principal del universo.\n",
    "    df = load_input_data(INPUT_PARQUET, EXPECTED_COLUMNS)\n",
    "    checks_df = run_data_quality_checks(df, EXPECTED_COLUMNS)\n",
    "    coverage_summary = build_coverage_summary(df)\n",
    "\n",
    "    # 2) Carga del benchmark para comparativas y metricas futuras.\n",
    "    spy_df, spy_check, spy_source = download_benchmark(\n",
    "        benchmark_ticker=PARAMS[\"benchmark_ticker\"],\n",
    "        start_date=PARAMS[\"backtest_start\"],\n",
    "        out_spy_parquet=OUT_SPY_PARQUET,\n",
    "    )\n",
    "    checks_df = pd.concat([checks_df, pd.DataFrame([spy_check])], ignore_index=True)\n",
    "\n",
    "    # 3) Parametros operativos centralizados en CSV (sin JSON global).\n",
    "    runtime_params = {\n",
    "        \"initial_capital\": PARAMS[\"initial_capital\"],\n",
    "        \"backtest_start\": PARAMS[\"backtest_start\"],\n",
    "        \"benchmark_ticker\": PARAMS[\"benchmark_ticker\"],\n",
    "        \"universe_rule\": PARAMS[\"universe_rule\"],\n",
    "        \"transaction_fee_rate\": PARAMS[\"transaction_fee_rate\"],\n",
    "        \"min_fee_per_order\": PARAMS[\"min_fee_per_order\"],\n",
    "        \"backtest_end_data\": coverage_summary[\"date_max\"],\n",
    "        \"input_parquet\": INPUT_PARQUET,\n",
    "        \"canonical_parquet\": OUT_CANONICAL,\n",
    "        \"benchmark_parquet\": OUT_SPY_PARQUET,\n",
    "        \"checks_csv\": OUT_CHECKS_CSV,\n",
    "        \"coverage_csv\": OUT_COVERAGE_CSV,\n",
    "        \"runtime_params_csv\": OUT_RUNTIME_PARAMS_CSV,\n",
    "        \"spy_source\": spy_source,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"pyarrow_version\": pa.__version__,\n",
    "        \"yfinance_version\": yf.__version__,\n",
    "    }\n",
    "\n",
    "    # 4) Exportes finales del notebook.\n",
    "    artifacts = export_artifacts(\n",
    "        canonical_df=df,\n",
    "        checks_df=checks_df,\n",
    "        coverage_summary=coverage_summary,\n",
    "        runtime_params=runtime_params,\n",
    "        out_canonical=OUT_CANONICAL,\n",
    "        out_checks_csv=OUT_CHECKS_CSV,\n",
    "        out_coverage_csv=OUT_COVERAGE_CSV,\n",
    "        out_runtime_csv=OUT_RUNTIME_PARAMS_CSV,\n",
    "    )\n",
    "\n",
    "    # 5) Checklist de readiness para desbloquear Notebook_2.\n",
    "    readiness_checklist = {\n",
    "        \"all_critical_checks_passed\": bool(checks_df[\"passed\"].all()),\n",
    "        \"canonical_dataset_created\": artifacts[\"canonical_exists\"],\n",
    "        \"benchmark_dataset_created\": artifacts[\"benchmark_exists\"],\n",
    "        \"checks_csv_created\": artifacts[\"checks_csv_exists\"],\n",
    "        \"coverage_csv_created\": artifacts[\"coverage_csv_exists\"],\n",
    "        \"runtime_params_csv_created\": artifacts[\"runtime_params_csv_exists\"],\n",
    "    }\n",
    "\n",
    "    readiness_df = pd.DataFrame(\n",
    "        [{\"item\": key, \"passed\": value} for key, value in readiness_checklist.items()]\n",
    "    )\n",
    "\n",
    "    fail_if(\n",
    "        not all(readiness_checklist.values()),\n",
    "        \"Data readiness FAILED: revisar checks y artefactos.\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"readiness_df\": readiness_df,\n",
    "        \"checks_df\": checks_df,\n",
    "        \"coverage_summary\": coverage_summary,\n",
    "        \"runtime_params\": runtime_params,\n",
    "        \"artifacts\": artifacts,\n",
    "        \"spy_rows\": int(len(spy_df)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Ejecucion principal del notebook.\n",
    "results = run_notebook_1()\n",
    "print(\"DATA READINESS: PASS. El proyecto puede continuar en Notebook_2.\")\n",
    "results[\"readiness_df\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54509643",
   "metadata": {},
   "source": [
    "## 5) Salidas de Control\n",
    "Se muestran checks, cobertura y estado de artefactos para revision rapida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ab1858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>passed</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>required_columns_present</td>\n",
       "      <td>True</td>\n",
       "      <td>missing_columns=[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_is_datetime</td>\n",
       "      <td>True</td>\n",
       "      <td>date dtype invalido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>symbol_is_string</td>\n",
       "      <td>True</td>\n",
       "      <td>symbol dtype invalido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>numeric_columns_dtype</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_numeric_dtype=[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duplicate_date_symbol</td>\n",
       "      <td>True</td>\n",
       "      <td>duplicate_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>critical_nulls</td>\n",
       "      <td>True</td>\n",
       "      <td>critical_null_count=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prices_positive</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_price_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volume_non_negative</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_volume_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ohlc_consistency</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_ohlc_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in_sp500_binary</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_in_sp500_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spy_required_columns_present</td>\n",
       "      <td>True</td>\n",
       "      <td>missing_columns=[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           check  passed                   details\n",
       "0       required_columns_present    True        missing_columns=[]\n",
       "1               date_is_datetime    True       date dtype invalido\n",
       "2               symbol_is_string    True     symbol dtype invalido\n",
       "3          numeric_columns_dtype    True  invalid_numeric_dtype=[]\n",
       "4          duplicate_date_symbol    True          duplicate_rows=0\n",
       "5                 critical_nulls    True     critical_null_count=0\n",
       "6                prices_positive    True      invalid_price_rows=0\n",
       "7            volume_non_negative    True     invalid_volume_rows=0\n",
       "8               ohlc_consistency    True       invalid_ohlc_rows=0\n",
       "9                in_sp500_binary    True   invalid_in_sp500_rows=0\n",
       "10  spy_required_columns_present    True        missing_columns=[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen de checks tecnicos ejecutados en N1.\n",
    "results[\"checks_df\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292d626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_min</th>\n",
       "      <th>date_max</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>total_symbols</th>\n",
       "      <th>symbols_on_last_date</th>\n",
       "      <th>active_symbols</th>\n",
       "      <th>inactive_symbols</th>\n",
       "      <th>sp500_members_min</th>\n",
       "      <th>sp500_members_median</th>\n",
       "      <th>sp500_members_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>2026-01-30</td>\n",
       "      <td>7250110</td>\n",
       "      <td>1289</td>\n",
       "      <td>650</td>\n",
       "      <td>650</td>\n",
       "      <td>639</td>\n",
       "      <td>498</td>\n",
       "      <td>500.0</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_min    date_max  total_rows  total_symbols  symbols_on_last_date  \\\n",
       "0  1990-01-02  2026-01-30     7250110           1289                   650   \n",
       "\n",
       "   active_symbols  inactive_symbols  sp500_members_min  sp500_members_median  \\\n",
       "0             650               639                498                 500.0   \n",
       "\n",
       "   sp500_members_max  \n",
       "0                507  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen de cobertura temporal y de universo.\n",
    "pd.DataFrame([results[\"coverage_summary\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca822c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_exists</th>\n",
       "      <th>benchmark_exists</th>\n",
       "      <th>checks_csv_exists</th>\n",
       "      <th>coverage_csv_exists</th>\n",
       "      <th>runtime_params_csv_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   canonical_exists  benchmark_exists  checks_csv_exists  coverage_csv_exists  \\\n",
       "0              True              True               True                 True   \n",
       "\n",
       "   runtime_params_csv_exists  \n",
       "0                       True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estado de artefactos exportados por el pipeline.\n",
    "pd.DataFrame([results[\"artifacts\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35b793",
   "metadata": {},
   "source": [
    "## Fin Notebook 1\n",
    "Todos los controles criticos deben quedar en `passed=True` para continuar en N2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
