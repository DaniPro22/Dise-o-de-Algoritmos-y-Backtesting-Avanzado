{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 - Carga de Datos\n",
    "\n",
    "## Objetivo y alcance\n",
    "- Que hace este notebook: cargar, validar tecnicamente y exportar datasets base.\n",
    "- Resultado esperado: un dataset canonico confiable para `Notebook_2` y artefactos de control reproducibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricciones tecnicas de librerias permitidas\n",
    "Librerias permitidas por el enunciado: `numpy`, `pandas`, `yfinance`, `matplotlib`, `seaborn`, `scipy`, `pyarrow`.\n",
    "Ademas se usa libreria estandar de Python: `pathlib`, `json`, `datetime`, `sys`, `platform`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa calculo numerico vectorizado (libreria permitida).\n",
    "import numpy as np\n",
    "# Importa estructuras tabulares y utilidades de datos (libreria permitida).\n",
    "import pandas as pd\n",
    "# Importa motor parquet permitido para lectura/escritura.\n",
    "import pyarrow as pa\n",
    "# Importa proveedor de datos de mercado permitido.\n",
    "import yfinance as yf\n",
    "# Importa manejo de rutas de forma portable.\n",
    "from pathlib import Path\n",
    "# Importa serializacion JSON para artefactos de config y reportes.\n",
    "import json\n",
    "# Importa fecha/hora con zona UTC para trazabilidad reproducible.\n",
    "from datetime import datetime, timezone\n",
    "# Importa informacion de version del interprete Python.\n",
    "import sys\n",
    "# Importa informacion de plataforma para trazabilidad del entorno.\n",
    "import platform\n",
    "\n",
    "# Define helper fail-fast para cortar ejecucion cuando una condicion invalida aparece.\n",
    "def fail_if(condition: bool, message: str) -> None:\n",
    "    \"\"\"Lanza ValueError si la condicion recibida es verdadera.\"\"\"\n",
    "    # Evalua la condicion de fallo.\n",
    "    if condition:\n",
    "        # Lanza error explicito para detener pipeline y evitar resultados corruptos.\n",
    "        raise ValueError(message)\n",
    "\n",
    "\n",
    "# Define helper para registrar checks y aplicar fail-fast en una sola llamada.\n",
    "def record_check(results: list, check_name: str, failed: bool, details: str) -> None:\n",
    "    \"\"\"Registra el estado de un check y detiene si falla.\"\"\"\n",
    "    # Calcula estado booleano de aprobacion.\n",
    "    passed = not failed\n",
    "    # Agrega una fila al reporte de validacion.\n",
    "    results.append({\"check\": check_name, \"passed\": passed, \"details\": details})\n",
    "    # Si el check fallo, corta ejecucion inmediatamente.\n",
    "    fail_if(failed, f\"[{check_name}] {details}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_capital': 250000,\n",
       " 'backtest_start': '2015-01-01',\n",
       " 'benchmark_ticker': 'SPY',\n",
       " 'universe_rule': 'in_sp500_point_in_time_last_13_months',\n",
       " 'transaction_fee_rate': 0.0023,\n",
       " 'min_fee_per_order': 23.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parametros maestros del proyecto como fuente unica.\n",
    "PARAMS = {\n",
    "    # Define capital inicial del enunciado.\n",
    "    \"initial_capital\": 250000,\n",
    "    # Define inicio oficial del backtest.\n",
    "    \"backtest_start\": \"2015-01-01\",\n",
    "    # Define ticker benchmark oficial.\n",
    "    \"benchmark_ticker\": \"SPY\",\n",
    "    # Define regla de universo point-in-time para evitar look-ahead.\n",
    "    \"universe_rule\": \"in_sp500_point_in_time_last_13_months\",\n",
    "    # Define coste proporcional de transaccion para etapas posteriores.\n",
    "    \"transaction_fee_rate\": 0.0023,\n",
    "    # Define coste minimo por orden para etapas posteriores.\n",
    "    \"min_fee_per_order\": 23.0,\n",
    "}\n",
    "\n",
    "# Muestra parametros para auditoria humana rapida.\n",
    "PARAMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_ts_utc': '2026-02-17T18:18:28.891764+00:00',\n",
       " 'input_parquet': 'data\\\\raw\\\\sp500_history.parquet',\n",
       " 'input_candidates': ['data\\\\raw\\\\sp500_history.parquet',\n",
       "  'C:\\\\Users\\\\dgall\\\\Downloads\\\\sp500_history.parquet'],\n",
       " 'out_canonical': 'data\\\\processed\\\\sp500_canonical.parquet',\n",
       " 'out_spy': 'data\\\\raw\\\\spy_yfinance.parquet',\n",
       " 'versions': {'python': '3.13.5',\n",
       "  'platform': 'Windows-11-10.0.26200-SP0',\n",
       "  'numpy': '2.1.3',\n",
       "  'pandas': '2.2.3',\n",
       "  'pyarrow': '19.0.0',\n",
       "  'yfinance': '1.1.0'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define raiz del proyecto como directorio actual resuelto.\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "# Define posibles ubicaciones del parquet de entrada con prioridad local.\n",
    "INPUT_CANDIDATES = [\n",
    "    # Prioriza una copia local dentro del proyecto para maxima portabilidad.\n",
    "    PROJECT_ROOT / \"data\" / \"raw\" / \"sp500_history.parquet\",\n",
    "    # Usa ruta de descarga del usuario como fallback de conveniencia.\n",
    "    Path.home() / \"Downloads\" / \"sp500_history.parquet\",\n",
    "]\n",
    "# Selecciona la primera ruta existente entre candidatas.\n",
    "INPUT_PARQUET = next((path for path in INPUT_CANDIDATES if path.exists()), None)\n",
    "# Detiene pipeline si no se encontro ningun input valido.\n",
    "fail_if(INPUT_PARQUET is None, f\"Input parquet no encontrado en: {[str(path) for path in INPUT_CANDIDATES]}\")\n",
    "\n",
    "# Define helper para guardar rutas relativas al proyecto cuando sea posible.\n",
    "def to_rel(path: Path) -> str:\n",
    "    \"\"\"Devuelve ruta relativa al proyecto; si no se puede, devuelve absoluta.\"\"\"\n",
    "    # Intenta convertir la ruta a relativa respecto al root del proyecto.\n",
    "    try:\n",
    "        # Retorna ruta relativa para mejorar portabilidad entre entornos.\n",
    "        return str(path.resolve().relative_to(PROJECT_ROOT))\n",
    "    # Si falla la conversion, usa ruta absoluta como fallback.\n",
    "    except ValueError:\n",
    "        # Retorna ruta absoluta para no perder trazabilidad.\n",
    "        return str(path.resolve())\n",
    "\n",
    "\n",
    "# Define carpeta de datos crudos para artefactos sin transformar en exceso.\n",
    "DIR_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "# Define carpeta de datos procesados para consumo del siguiente notebook.\n",
    "DIR_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "# Define carpeta de configuracion comun entre notebooks.\n",
    "DIR_CONFIG = PROJECT_ROOT / \"config\"\n",
    "# Define carpeta de reportes tecnicos de validacion.\n",
    "DIR_REPORTS = PROJECT_ROOT / \"reports\"\n",
    "# Crea carpetas si no existen para asegurar estructura estable del pipeline.\n",
    "for directory in [DIR_RAW, DIR_PROCESSED, DIR_CONFIG, DIR_REPORTS]:\n",
    "    # Crea directorio y padres necesarios sin error si ya existe.\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define salida canonical para N2.\n",
    "OUT_CANONICAL = DIR_PROCESSED / \"sp500_canonical.parquet\"\n",
    "# Define salida benchmark SPY descargado.\n",
    "OUT_SPY = DIR_RAW / \"spy_yfinance.parquet\"\n",
    "# Define salida de config comun para N1-N5.\n",
    "OUT_CONFIG = DIR_CONFIG / \"config.json\"\n",
    "# Define salida de reporte tecnico JSON.\n",
    "OUT_REPORT_JSON = DIR_REPORTS / \"data_quality_n1.json\"\n",
    "# Define salida de reporte tecnico CSV.\n",
    "OUT_REPORT_CSV = DIR_REPORTS / \"data_quality_n1_checks.csv\"\n",
    "# Define salida de resumen de cobertura en CSV.\n",
    "OUT_COVERAGE_CSV = DIR_REPORTS / \"data_quality_n1_coverage.csv\"\n",
    "\n",
    "# Registra timestamp UTC de ejecucion para reproducibilidad.\n",
    "RUN_TS_UTC = datetime.now(timezone.utc).isoformat()\n",
    "# Registra versiones tecnicas clave del entorno.\n",
    "VERSIONS = {\n",
    "    # Registra version de Python.\n",
    "    \"python\": sys.version.split()[0],\n",
    "    # Registra version de plataforma.\n",
    "    \"platform\": platform.platform(),\n",
    "    # Registra version de numpy.\n",
    "    \"numpy\": np.__version__,\n",
    "    # Registra version de pandas.\n",
    "    \"pandas\": pd.__version__,\n",
    "    # Registra version de pyarrow.\n",
    "    \"pyarrow\": pa.__version__,\n",
    "    # Registra version de yfinance.\n",
    "    \"yfinance\": yf.__version__,\n",
    "}\n",
    "\n",
    "# Construye bloque de trazabilidad minima requerido.\n",
    "TRACEABILITY = {\n",
    "    # Guarda timestamp UTC de ejecucion.\n",
    "    \"run_ts_utc\": RUN_TS_UTC,\n",
    "    # Guarda input seleccionado para ejecucion.\n",
    "    \"input_parquet\": to_rel(INPUT_PARQUET),\n",
    "    # Guarda listado de rutas candidatas de input.\n",
    "    \"input_candidates\": [to_rel(path) for path in INPUT_CANDIDATES],\n",
    "    # Guarda ruta de salida canonical.\n",
    "    \"out_canonical\": to_rel(OUT_CANONICAL),\n",
    "    # Guarda ruta de salida benchmark.\n",
    "    \"out_spy\": to_rel(OUT_SPY),\n",
    "    # Guarda versiones de librerias y entorno.\n",
    "    \"versions\": VERSIONS,\n",
    "}\n",
    "\n",
    "# Muestra trazabilidad para auditoria.\n",
    "TRACEABILITY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7250110, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define columnas esperadas para carga robusta y consistente.\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"date\",\n",
    "    \"symbol\",\n",
    "    \"assetid\",\n",
    "    \"security_name\",\n",
    "    \"sector\",\n",
    "    \"industry\",\n",
    "    \"subsector\",\n",
    "    \"in_sp500\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"unadjusted_close\",\n",
    "]\n",
    "\n",
    "# Verifica que el parquet de entrada exista antes de leer.\n",
    "fail_if(not INPUT_PARQUET.exists(), f\"Input parquet no encontrado: {INPUT_PARQUET}\")\n",
    "# Lee parquet con seleccion explicita de columnas esperadas.\n",
    "df = pd.read_parquet(INPUT_PARQUET, columns=EXPECTED_COLUMNS)\n",
    "# Normaliza nombres de columnas a minusculas para homogeneidad.\n",
    "df.columns = [str(col).lower() for col in df.columns]\n",
    "# Convierte columna date a datetime y marca errores como NaT para validacion posterior.\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "# Fuerza symbol a dtype string de pandas para estabilidad en merges y filtros.\n",
    "df[\"symbol\"] = df[\"symbol\"].astype(\"string\")\n",
    "# Ordena por fecha y simbolo para garantizar consistencia temporal.\n",
    "df = df.sort_values([\"date\", \"symbol\"]).reset_index(drop=True)\n",
    "\n",
    "# Muestra forma del dataset base cargado.\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>passed</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>required_columns_present</td>\n",
       "      <td>True</td>\n",
       "      <td>missing_columns=[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_is_datetime</td>\n",
       "      <td>True</td>\n",
       "      <td>date dtype invalido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>symbol_is_string</td>\n",
       "      <td>True</td>\n",
       "      <td>symbol dtype invalido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>numeric_columns_dtype</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_numeric_dtype=[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duplicate_date_symbol</td>\n",
       "      <td>True</td>\n",
       "      <td>duplicate_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>critical_nulls</td>\n",
       "      <td>True</td>\n",
       "      <td>critical_null_count=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prices_positive</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_price_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volume_non_negative</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_volume_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ohlc_consistency</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_ohlc_rows=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in_sp500_binary</td>\n",
       "      <td>True</td>\n",
       "      <td>invalid_in_sp500_rows=0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      check  passed                   details\n",
       "0  required_columns_present    True        missing_columns=[]\n",
       "1          date_is_datetime    True       date dtype invalido\n",
       "2          symbol_is_string    True     symbol dtype invalido\n",
       "3     numeric_columns_dtype    True  invalid_numeric_dtype=[]\n",
       "4     duplicate_date_symbol    True          duplicate_rows=0\n",
       "5            critical_nulls    True     critical_null_count=0\n",
       "6           prices_positive    True      invalid_price_rows=0\n",
       "7       volume_non_negative    True     invalid_volume_rows=0\n",
       "8          ohlc_consistency    True       invalid_ohlc_rows=0\n",
       "9           in_sp500_binary    True   invalid_in_sp500_rows=0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa contenedor de checks tecnicos.\n",
    "validation_rows = []\n",
    "\n",
    "# Verifica presencia de todas las columnas requeridas.\n",
    "missing_columns = sorted(set(EXPECTED_COLUMNS) - set(df.columns))\n",
    "# Registra check de columnas obligatorias.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"required_columns_present\",\n",
    "    bool(missing_columns),\n",
    "    f\"missing_columns={missing_columns}\",\n",
    ")\n",
    "\n",
    "# Verifica tipo datetime en date.\n",
    "date_is_datetime = pd.api.types.is_datetime64_any_dtype(df[\"date\"])\n",
    "# Registra check de tipo para date.\n",
    "record_check(validation_rows, \"date_is_datetime\", not date_is_datetime, \"date dtype invalido\")\n",
    "\n",
    "# Verifica tipo string para symbol.\n",
    "symbol_is_string = pd.api.types.is_string_dtype(df[\"symbol\"])\n",
    "# Registra check de tipo para symbol.\n",
    "record_check(validation_rows, \"symbol_is_string\", not symbol_is_string, \"symbol dtype invalido\")\n",
    "\n",
    "# Define columnas que deben ser numericas.\n",
    "numeric_columns = [\"in_sp500\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"unadjusted_close\"]\n",
    "# Detecta columnas numericas con tipo incorrecto.\n",
    "numeric_type_failures = [col for col in numeric_columns if not pd.api.types.is_numeric_dtype(df[col])]\n",
    "# Registra check de tipos numericos.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"numeric_columns_dtype\",\n",
    "    bool(numeric_type_failures),\n",
    "    f\"invalid_numeric_dtype={numeric_type_failures}\",\n",
    ")\n",
    "\n",
    "# Calcula duplicados de clave primaria tecnica date+symbol.\n",
    "duplicate_count = int(df.duplicated([\"date\", \"symbol\"]).sum())\n",
    "# Registra check de duplicados clave.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"duplicate_date_symbol\",\n",
    "    duplicate_count > 0,\n",
    "    f\"duplicate_rows={duplicate_count}\",\n",
    ")\n",
    "\n",
    "# Define columnas criticas para continuidad del pipeline.\n",
    "critical_columns = [\"date\", \"symbol\", \"in_sp500\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "# Calcula nulos sobre columnas criticas.\n",
    "critical_null_count = int(df[critical_columns].isna().sum().sum())\n",
    "# Registra check de nulos criticos.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"critical_nulls\",\n",
    "    critical_null_count > 0,\n",
    "    f\"critical_null_count={critical_null_count}\",\n",
    ")\n",
    "\n",
    "# Calcula filas con precios OHLC no positivos.\n",
    "invalid_price_count = int((df[[\"open\", \"high\", \"low\", \"close\"]] <= 0).any(axis=1).sum())\n",
    "# Registra check de precios positivos.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"prices_positive\",\n",
    "    invalid_price_count > 0,\n",
    "    f\"invalid_price_rows={invalid_price_count}\",\n",
    ")\n",
    "\n",
    "# Calcula filas con volumen negativo.\n",
    "invalid_volume_count = int((df[\"volume\"] < 0).sum())\n",
    "# Registra check de volumen no negativo.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"volume_non_negative\",\n",
    "    invalid_volume_count > 0,\n",
    "    f\"invalid_volume_rows={invalid_volume_count}\",\n",
    ")\n",
    "\n",
    "# Calcula condicion de OHLC incoherente por limite superior.\n",
    "high_inconsistency = df[\"high\"] < df[[\"open\", \"close\", \"low\"]].max(axis=1)\n",
    "# Calcula condicion de OHLC incoherente por limite inferior.\n",
    "low_inconsistency = df[\"low\"] > df[[\"open\", \"close\", \"high\"]].min(axis=1)\n",
    "# Calcula total de inconsistencias OHLC.\n",
    "invalid_ohlc_count = int((high_inconsistency | low_inconsistency).sum())\n",
    "# Registra check de coherencia OHLC.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"ohlc_consistency\",\n",
    "    invalid_ohlc_count > 0,\n",
    "    f\"invalid_ohlc_rows={invalid_ohlc_count}\",\n",
    ")\n",
    "\n",
    "# Calcula filas con valores de in_sp500 fuera de {0, 1}.\n",
    "invalid_in_sp500_count = int((~df[\"in_sp500\"].isin([0, 1])).sum())\n",
    "# Registra check de dominio binario para in_sp500.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"in_sp500_binary\",\n",
    "    invalid_in_sp500_count > 0,\n",
    "    f\"invalid_in_sp500_rows={invalid_in_sp500_count}\",\n",
    ")\n",
    "\n",
    "# Convierte resultados de checks a dataframe para export y lectura humana.\n",
    "checks_df = pd.DataFrame(validation_rows)\n",
    "# Muestra resumen de validaciones tecnicas.\n",
    "checks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_min': '1990-01-02',\n",
       " 'date_max': '2026-01-30',\n",
       " 'total_rows': 7250110,\n",
       " 'total_symbols': 1289,\n",
       " 'symbols_on_last_date': 650,\n",
       " 'active_symbols': 650,\n",
       " 'inactive_symbols': 639,\n",
       " 'sp500_members_min': 498,\n",
       " 'sp500_members_median': 500.0,\n",
       " 'sp500_members_max': 507}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula fecha minima del dataset cargado.\n",
    "date_min = df[\"date\"].min()\n",
    "# Calcula fecha maxima del dataset cargado.\n",
    "date_max = df[\"date\"].max()\n",
    "# Calcula numero total de simbolos historicos.\n",
    "total_symbols = int(df[\"symbol\"].nunique())\n",
    "# Calcula ultimo dia disponible del dataset.\n",
    "last_data_date = date_max\n",
    "# Calcula simbolos con datos en la fecha maxima.\n",
    "symbols_on_last_date = int(df.loc[df[\"date\"] == last_data_date, \"symbol\"].nunique())\n",
    "# Calcula ultima fecha observada por simbolo para estimar actividad.\n",
    "last_date_by_symbol = df.groupby(\"symbol\", observed=True)[\"date\"].max()\n",
    "# Calcula simbolos activos en la ultima fecha disponible.\n",
    "active_symbols = int((last_date_by_symbol == last_data_date).sum())\n",
    "# Calcula simbolos historicos que ya no llegan a la ultima fecha.\n",
    "inactive_symbols = int(total_symbols - active_symbols)\n",
    "# Calcula serie diaria del conteo de miembros S&P por flag point-in-time.\n",
    "sp500_daily_count = (\n",
    "    df.loc[df[\"in_sp500\"] == 1]\n",
    "    .groupby(\"date\", observed=True)[\"symbol\"]\n",
    "    .nunique()\n",
    "    .rename(\"sp500_members\")\n",
    ")\n",
    "\n",
    "# Construye resumen compacto de cobertura basica.\n",
    "coverage_summary = {\n",
    "    \"date_min\": date_min.date().isoformat(),\n",
    "    \"date_max\": date_max.date().isoformat(),\n",
    "    \"total_rows\": int(len(df)),\n",
    "    \"total_symbols\": total_symbols,\n",
    "    \"symbols_on_last_date\": symbols_on_last_date,\n",
    "    \"active_symbols\": active_symbols,\n",
    "    \"inactive_symbols\": inactive_symbols,\n",
    "    \"sp500_members_min\": int(sp500_daily_count.min()),\n",
    "    \"sp500_members_median\": float(sp500_daily_count.median()),\n",
    "    \"sp500_members_max\": int(sp500_daily_count.max()),\n",
    "}\n",
    "\n",
    "# Muestra resumen de cobertura para control rapido.\n",
    "coverage_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota metodologica: point-in-time y sesgo de supervivencia\n",
    "- El universo se construira por fecha usando `in_sp500` en cada momento historico (point-in-time).\n",
    "- Se conservan simbolos que dejaron de cotizar o salieron del indice para no introducir sesgo de supervivencia.\n",
    "- El objetivo es evitar usar informacion futura que no estaria disponible en cada fecha de decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convencion de precios para etapas posteriores\n",
    "- Se√±ales de momentum: se usara `close` (ajustado) para retornos comparables historicamente.\n",
    "- Ejecucion de rebalanceo: se usaran `open` y `close` segun protocolo del enunciado.\n",
    "- Auditoria: se conserva `unadjusted_close` para trazabilidad y comprobaciones adicionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>SPY</td>\n",
       "      <td>206.380005</td>\n",
       "      <td>206.880005</td>\n",
       "      <td>204.179993</td>\n",
       "      <td>205.429993</td>\n",
       "      <td>121465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>SPY</td>\n",
       "      <td>204.169998</td>\n",
       "      <td>204.369995</td>\n",
       "      <td>201.350006</td>\n",
       "      <td>201.720001</td>\n",
       "      <td>169632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>SPY</td>\n",
       "      <td>202.089996</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>198.860001</td>\n",
       "      <td>199.820007</td>\n",
       "      <td>209151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>SPY</td>\n",
       "      <td>201.419998</td>\n",
       "      <td>202.720001</td>\n",
       "      <td>200.880005</td>\n",
       "      <td>202.309998</td>\n",
       "      <td>125346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>SPY</td>\n",
       "      <td>204.009995</td>\n",
       "      <td>206.160004</td>\n",
       "      <td>203.990005</td>\n",
       "      <td>205.899994</td>\n",
       "      <td>147217800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date symbol        open        high         low       close     volume\n",
       "0 2015-01-02    SPY  206.380005  206.880005  204.179993  205.429993  121465900\n",
       "1 2015-01-05    SPY  204.169998  204.369995  201.350006  201.720001  169632600\n",
       "2 2015-01-06    SPY  202.089996  202.720001  198.860001  199.820007  209151400\n",
       "3 2015-01-07    SPY  201.419998  202.720001  200.880005  202.309998  125346700\n",
       "4 2015-01-08    SPY  204.009995  206.160004  203.990005  205.899994  147217800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa fuente de benchmark para trazabilidad operativa.\n",
    "spy_source = \"yfinance_download\"\n",
    "# Intenta descargar benchmark SPY desde yfinance.\n",
    "try:\n",
    "    # Descarga OHLCV desde inicio de backtest con ajuste desactivado.\n",
    "    spy_raw = yf.download(\n",
    "        tickers=PARAMS[\"benchmark_ticker\"],\n",
    "        start=PARAMS[\"backtest_start\"],\n",
    "        progress=False,\n",
    "        auto_adjust=False,\n",
    "    )\n",
    "    # Verifica que la descarga devolvio datos.\n",
    "    fail_if(spy_raw.empty, \"No se pudo descargar SPY desde yfinance.\")\n",
    "    # Si yfinance devuelve columnas multinivel, aplana al primer nivel.\n",
    "    if isinstance(spy_raw.columns, pd.MultiIndex):\n",
    "        # Conserva solo nombre de campo OHLCV del primer nivel.\n",
    "        spy_raw.columns = [str(col[0]).lower().replace(\" \", \"_\") for col in spy_raw.columns]\n",
    "    else:\n",
    "        # Normaliza columnas simples a minusculas y snake_case.\n",
    "        spy_raw.columns = [str(col).lower().replace(\" \", \"_\") for col in spy_raw.columns]\n",
    "    # Mueve el indice Date a columna para homogeneizar con el parquet principal.\n",
    "    spy_df = spy_raw.reset_index()\n",
    "    # Normaliza nombres de columnas tras reset del indice.\n",
    "    spy_df.columns = [str(col).lower().replace(\" \", \"_\") for col in spy_df.columns]\n",
    "    # Fuerza parseo de fecha para control de calidad basico.\n",
    "    spy_df[\"date\"] = pd.to_datetime(spy_df[\"date\"], errors=\"coerce\")\n",
    "    # Inserta identificador de simbolo benchmark.\n",
    "    spy_df[\"symbol\"] = PARAMS[\"benchmark_ticker\"]\n",
    "# Si falla la descarga, usa fallback local para permitir re-ejecucion sin red.\n",
    "except Exception as exc:\n",
    "    # Marca fuente fallback para trazabilidad.\n",
    "    spy_source = \"local_parquet_fallback\"\n",
    "    # Detiene pipeline si tampoco existe benchmark local previo.\n",
    "    fail_if(not OUT_SPY.exists(), f\"No se pudo descargar SPY y no existe fallback local: {exc}\")\n",
    "    # Carga benchmark local previamente guardado.\n",
    "    spy_df = pd.read_parquet(OUT_SPY)\n",
    "\n",
    "# Define columnas minimas requeridas para benchmark base.\n",
    "spy_required_columns = [\"date\", \"symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "# Detecta columnas faltantes en benchmark.\n",
    "spy_missing_columns = sorted(set(spy_required_columns) - set(spy_df.columns))\n",
    "# Registra check tecnico del benchmark disponible.\n",
    "record_check(\n",
    "    validation_rows,\n",
    "    \"spy_required_columns_present\",\n",
    "    bool(spy_missing_columns),\n",
    "    f\"spy_missing_columns={spy_missing_columns}\",\n",
    ")\n",
    "# Fuerza parseo de fecha en benchmark por consistencia.\n",
    "spy_df[\"date\"] = pd.to_datetime(spy_df[\"date\"], errors=\"coerce\")\n",
    "# Selecciona columnas canonical del benchmark y ordena por fecha.\n",
    "spy_df = spy_df[spy_required_columns].sort_values(\"date\").reset_index(drop=True)\n",
    "# Guarda benchmark en parquet para notebooks posteriores.\n",
    "spy_df.to_parquet(OUT_SPY, index=False)\n",
    "# Registra fuente final de benchmark en trazabilidad.\n",
    "TRACEABILITY[\"spy_source\"] = spy_source\n",
    "\n",
    "# Muestra muestra corta del benchmark cargado.\n",
    "spy_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical_exists': True,\n",
       " 'spy_exists': True,\n",
       " 'config_exists': True,\n",
       " 'quality_json_exists': True,\n",
       " 'quality_csv_exists': True,\n",
       " 'coverage_csv_exists': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guarda dataset canonico validado para Notebook 2.\n",
    "df.to_parquet(OUT_CANONICAL, index=False)\n",
    "\n",
    "# Actualiza parametros con fecha final real disponible en datos.\n",
    "params_with_data_end = {\n",
    "    **PARAMS,\n",
    "    \"backtest_end_data\": date_max.date().isoformat(),\n",
    "}\n",
    "\n",
    "# Construye payload de configuracion comun para N1-N5.\n",
    "config_payload = {\n",
    "    \"project\": \"diseno_algoritmos_y_backtesting_avanzado\",\n",
    "    \"params\": params_with_data_end,\n",
    "    \"paths\": {\n",
    "        \"input_parquet\": to_rel(INPUT_PARQUET),\n",
    "        \"canonical_parquet\": to_rel(OUT_CANONICAL),\n",
    "        \"benchmark_parquet\": to_rel(OUT_SPY),\n",
    "        \"quality_json\": to_rel(OUT_REPORT_JSON),\n",
    "        \"quality_csv\": to_rel(OUT_REPORT_CSV),\n",
    "        \"coverage_csv\": to_rel(OUT_COVERAGE_CSV),\n",
    "    },\n",
    "    \"price_conventions\": {\n",
    "        \"signals_price\": \"close\",\n",
    "        \"execution_prices\": [\"open\", \"close\"],\n",
    "        \"audit_price\": \"unadjusted_close\",\n",
    "    },\n",
    "    \"traceability\": TRACEABILITY,\n",
    "}\n",
    "# Escribe config comun en formato JSON legible.\n",
    "OUT_CONFIG.write_text(json.dumps(config_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Reconstruye dataframe de checks por si se agregaron checks posteriores (ej: SPY).\n",
    "checks_df = pd.DataFrame(validation_rows)\n",
    "# Construye payload de reporte tecnico de calidad.\n",
    "quality_report_payload = {\n",
    "    \"run_ts_utc\": RUN_TS_UTC,\n",
    "    \"traceability\": TRACEABILITY,\n",
    "    \"coverage_summary\": coverage_summary,\n",
    "    \"checks\": checks_df.to_dict(orient=\"records\"),\n",
    "}\n",
    "# Escribe reporte JSON de validacion tecnica.\n",
    "OUT_REPORT_JSON.write_text(json.dumps(quality_report_payload, indent=2), encoding=\"utf-8\")\n",
    "# Escribe tabla CSV de checks tecnicos.\n",
    "checks_df.to_csv(OUT_REPORT_CSV, index=False)\n",
    "# Escribe tabla CSV de cobertura basica.\n",
    "pd.DataFrame([coverage_summary]).to_csv(OUT_COVERAGE_CSV, index=False)\n",
    "\n",
    "# Muestra resumen de artefactos exportados.\n",
    "artifacts_summary = {\n",
    "    \"canonical_exists\": OUT_CANONICAL.exists(),\n",
    "    \"spy_exists\": OUT_SPY.exists(),\n",
    "    \"config_exists\": OUT_CONFIG.exists(),\n",
    "    \"quality_json_exists\": OUT_REPORT_JSON.exists(),\n",
    "    \"quality_csv_exists\": OUT_REPORT_CSV.exists(),\n",
    "    \"coverage_csv_exists\": OUT_COVERAGE_CSV.exists(),\n",
    "}\n",
    "# Muestra estado de exportes.\n",
    "artifacts_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA READINESS: PASS. El proyecto puede continuar en Notebook_2.\n"
     ]
    }
   ],
   "source": [
    "# Construye checklist final de readiness para permitir pasar a Notebook 2.\n",
    "readiness_checklist = {\n",
    "    \"all_critical_checks_passed\": bool(checks_df[\"passed\"].all()),\n",
    "    \"canonical_dataset_created\": OUT_CANONICAL.exists(),\n",
    "    \"benchmark_dataset_created\": OUT_SPY.exists(),\n",
    "    \"config_json_created\": OUT_CONFIG.exists(),\n",
    "    \"quality_json_created\": OUT_REPORT_JSON.exists(),\n",
    "    \"quality_csv_created\": OUT_REPORT_CSV.exists(),\n",
    "}\n",
    "# Convierte checklist a dataframe para lectura clara en notebook.\n",
    "readiness_df = pd.DataFrame(\n",
    "    [{\"item\": key, \"passed\": value} for key, value in readiness_checklist.items()]\n",
    ")\n",
    "# Muestra checklist final.\n",
    "readiness_df\n",
    "\n",
    "# Corta ejecucion si el notebook no queda apto para continuar pipeline.\n",
    "fail_if(not all(readiness_checklist.values()), \"Data readiness FAILED: revisar checks y artefactos.\")\n",
    "# Imprime mensaje final de aprobacion formal.\n",
    "print(\"DATA READINESS: PASS. El proyecto puede continuar en Notebook_2.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
